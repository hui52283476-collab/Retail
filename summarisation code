from transformers import pipeline
import nltk
from nltk.tokenize import sent_tokenize

def summarize_text(text, max_length=150, min_length=30):
    summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
    
    if len(text.split()) > 1024:
        sentences = sent_tokenize(text)
        chunks = []
        current_chunk = []
        current_length = 0
        
        for sentence in sentences:
            sentence_length = len(sentence.split())
            if current_length + sentence_length <= 800:
                current_chunk.append(sentence)
                current_length += sentence_length
            else:
                chunks.append(" ".join(current_chunk))
                current_chunk = [sentence]
                current_length = sentence_length
        
        if current_chunk:
            chunks.append(" ".join(current_chunk))
        
        summaries = []
        for chunk in chunks:
            summary = summarizer(chunk, max_length=max_length, min_length=min_length, do_sample=False)
            summaries.append(summary[0]['summary_text'])
        
        if len(summaries) > 1:
            combined_text = " ".join(summaries)
            final_summary = summarizer(combined_text, max_length=max_length, min_length=min_length, do_sample=False)
            return final_summary[0]['summary_text']
        else:
            return summaries[0]
    else:
        summary = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)
        return summary[0]['summary_text']

# Get input text
input_text = input("Paste your text here: ")

# Generate summary
summary = summarize_text(input_text)
print("\nSummary:", summary)
